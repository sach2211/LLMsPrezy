<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>From Words to Meaning: The Evolution to LLMs</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/theme/black.css" id="theme">
    <!-- Highlight.js for code blocks -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- FIX: Added the Reveal.js highlight plugin script -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/highlight/highlight.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;700&family=Kalam:wght@400&display=swap');
        body {
            font-family: 'Inter', sans-serif;
        }

        .reveal .slides section h1,
        .reveal .slides section h2,
        .reveal .slides section h3,
        .reveal .slides section h4 {
            font-family: 'Inter', sans-serif;
            text-transform: none;
        }

        .reveal .slides section .title-large {
            font-size: 4vw;
            text-transform: uppercase;
            font-weight: 700;
            letter-spacing: 0.2vw;
            color: #4dc2f5; /* A vibrant blue accent */
            text-shadow: 0.2vw 0.2vw 0.5vw rgba(0,0,0,0.5);
            line-height: 1.1;
            margin: 0.2em 0;
        }

        .reveal .slides section p {
            line-height: 1.4;
        }
        
        .reveal .slides section .hook-text {
            color: #f5f5f5;
            font-size: 1.5em;
            line-height: 1.2;
            font-weight: 300;
            text-align: center;
        }

        .reveal pre {
            border: 0.1vw solid #4dc2f5;
            box-shadow: 0 0 1vw rgba(77, 194, 245, 0.5);
            padding: 1.5vw;
            border-radius: 0.8vw;
        }
        
        .reveal code {
            font-size: 0.9em;
            background-color: #1a1a1a;
            border-radius: 0.5vw;
        }

        .reveal .progress span {
            background: #4dc2f5;
        }

        .reveal .controls button {
            color: #4dc2f5;
        }

        .comparison-table {
            width: 100%;
            margin: 20px 0;
            border-collapse: collapse;
            font-size: 0.72em;
        }

        .comparison-table th, .comparison-table td {
            border: 0.1vw solid #444;
            padding: 0.6vw 1vw;
            text-align: left;
            white-space: nowrap;
            vertical-align: top;
        }

        .comparison-table th {
            background-color: #222;
            color: #4dc2f5;
            font-weight: 700;
        }

        .comparison-table td {
            background-color: #1a1a1a;
        }
        
        .code-title {
            text-align: left;
            font-size: 0.8em;
            color: #aaa;
            margin-bottom: 5px;
        }

        /* Word2Vec Vector Visualization Styles */
        .viz-container {
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin-top: 3vh;
            scale: 1.2;
        }

        .vector-label {
            font-family: 'Kalam', cursive;
            font-size: 2em;
        }

        .vector {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: 'Kalam', cursive;
            font-size: 1.8em;
        }

        .vector-dim {
            padding: 5px 15px;
            margin: 2px 0;
            border: 3px solid transparent;
            border-radius: 50%;
            transition: all 0.3s ease-in-out;
        }
        
        .vector-dim.highlight {
            border-color: #EF4444;
        }

        .ellipsis {
            letter-spacing: 5px;
            padding: 10px 0;
        }

        .bracket {
            font-size: 8em;
            font-weight: 100;
            color: #9CA3AF;
        }

        .annotation {
            position: absolute;
            font-family: 'Kalam', cursive;
            font-size: 1.5em;
            opacity: 0;
            visibility: hidden;
            transition: all 0.4s ease-in-out;
            color: #9CA3AF;
        }

        .annotation.visible {
            opacity: 1;
            visibility: visible;
        }
        
        #anno-authority {
            top: 65px;
            left: 65%;
        }

        #anno-power {
            bottom: 15%;
            left: 65%;
        }
        
        .annotation svg {
            position: absolute;
            top: 50%;
            right: 100%;
            width: 100px;
            height: 50px;
            transform: translateY(-50%);
            overflow: visible;
        }

        #anno-power svg {
            width: 150px;
        }

        .annotation path {
            stroke: #9CA3AF;
            stroke-width: 2;
            fill: none;
            stroke-dasharray: 200;
            stroke-dashoffset: 200;
            transition: stroke-dashoffset 0.8s ease-in-out;
        }

        .annotation.visible path {
            stroke-dashoffset: 0;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Opening Hook (2 minutes) -->
            <section data-transition="zoom">
                <!-- Brain image in top right corner -->
                <div style="position: absolute; top: 2vh; right: 2vw; z-index: 10;">
                    <img src="./llm_brain_title.jpg" alt="LLM Brain Visualization" style="width: 15vw; height: auto; border-radius: 1vw; box-shadow: 0 0 1.5vw rgba(77, 194, 245, 0.3); border: 0.1vw solid #4dc2f5; opacity: 0.8;">
                </div>
                
                <h2 class="title-large">The Road  &nbsp;</h2>
                <h2 class="title-large">to LLMs</h2>
                <h3>From a Single Neuron to a Global Brain</h3>
                <p class="fragment fade-in-then-out">Singapore Technology Week 2025</p>
                
                <aside class="notes">
                    Hello everyone, I'm thrilled to be here. Today, we're going to explore one of the most incredible transformations in recent memory - the evolution from simple neurons to complex language models.
                </aside>
            </section>
            <section data-transition="slide">
                <p class="hook-text">About Me</p>
                <div style="display: flex; align-items: center; justify-content: center; gap: 4vw;">
                    <div style="flex: 1; text-align: center;">
                        <img src="./SachinProfile.jpg" alt="Sachin Chopra" style="width: 12vw; height: 12vw; border-radius: 50%; object-fit: cover; border: 0.3vw solid #4dc2f5; box-shadow: 0 0 2vw rgba(77, 194, 245, 0.3);">
                    </div>
                    <div style="flex: 2;">
                        <ul style="text-align: left;">
                            <li>Sachin Chopra</li>
                            <li>Lead Full Stack Engineer at Grab</li>
                            <li>Passionate about the evolution of AI and language models</li>
                        </ul>
                    </div>
                </div>
            </section>
            <section data-transition="slide">
                <p class="hook-text">About the Talk</p>
                <ul>
                    <li>I'll be talking about the evolution of language models from N-grams to LLMs</li>
                    <li>I'll be talking about the history of language models</li>
                    <li>I'll be talking about the current state of language models</li>
                </ul>
            </section>
            <section data-transition="slide">
                <h2 class="hook-text">How did we go from a simple autocomplete...</h2>
                <div class="fragment fade-in" style="width: 100%; margin: 0 auto;">
                    <pre><code class="plaintext">The cat sat on the ___
-> mat
-> rug
-> sofa</code></pre>
                </div>
                <p class="fragment fade-in" style="font-size: 1.2em; color: #aaa; margin-top: 3vh;">Simple pattern matching based on frequency</p>
            </section>
            
            <section data-transition="slide">
                <h2 class="hook-text">...to machines that seem to understand context, meaning, and even reasoning?</h2>
                <div class="fragment fade-in" style="width: 100%; margin: 0 auto;">
                    <pre><code class="plaintext">"The bank can guarantee deposits will eventually cover 
future tuition costs, but only after you've paid off 
your river canoe."

-> A nuanced, contextual response that understands 
   multiple meanings of 'bank'</code></pre>
                </div>
                <p class="fragment fade-in" style="font-size: 1.2em; color: #4dc2f5; margin-top: 3vh;">Complex reasoning across long contexts</p>
            </section>

            <!-- I. The Foundation Era: Statistical NLP (5 minutes) -->
            <section>
                <h2>I. The Foundation Era: Statistical NLP</h2>
                <p class="fragment fade-in">1950s-2000s</p>
            </section>

            <section data-transition="slide">
                <h3>N-gram Models: The Counting Approach</h3>
                <p class="fragment fade-in">Core principle based on frequency counting:</p>
                <pre class="fragment fade-in"><code class="latex">P(word | previous n-1 words)</code></pre>
                <pre class="fragment fade-in"><code class="python"># Simple N-gram counting
data = ["the cat sat on the mat", "the dog sat on the rug"]
counts = {}
for sentence in data:
    words = sentence.split()
    for i in range(len(words) - 1):
        bigram = (words[i], words[i+1])
        counts[bigram] = counts.get(bigram, 0) + 1

print(counts)
# Output: {('the', 'cat'): 1, ('cat', 'sat'): 1, ...}</code></pre>
            </section>

            <section data-transition="slide">
                <h3>N-gram Strengths & Fatal Flaws</h3>
                <ul style="width: 100%; margin: 0 auto;">
                    <li class="fragment fade-in"><strong>Strengths:</strong> Simple, interpretable, computationally efficient.</li>
                    <li class="fragment fade-in"><strong>Limitations:</strong>
                        <ul class="fragment fade-in">
                            <li><span class="fragment highlight-red">Fixed context window</span> (typically 3-5 words)</li>
                            <li><span class="fragment highlight-red">Data sparsity problem:</span> What about phrases never seen before?</li>
                            <li><span class="fragment highlight-red">No semantic understanding:</span> It's just a lookup table.</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section data-transition="slide">
                <h3>A Live Demo of N-gram Failure</h3>
                
                <p class="fragment fade-in">Input sentence:</p>
                <div class="fragment fade-in" style="width: 100%; margin: 0 auto;">
                    <pre><code class="plaintext">"The bank can guarantee deposits will eventually cover 
future tuition costs, but only after you've paid off 
your river canoe."</code></pre>
                </div>
                
                <p class="fragment fade-in" style="margin-top: 3vh;">N-gram model processing:</p>
                <div class="fragment fade-in" style="width: 100%; margin: 0 auto;">
                    <pre><code class="plaintext">Processing: "paid off your river..."
Next word prediction: ???</code></pre>
                </div>
                
                <p class="fragment fade-in" style="font-size: 1.3em; color: #ff6b6b; margin-top: 3vh; font-weight: bold;">
                    ❌ The model has already forgotten "bank" from the beginning!
                </p>
            </section>

            <!-- II. The Neural Revolution: Learning Representations (8 minutes) -->
            <section>
                <h2>II. The Neural Revolution: Learning Representations</h2>
                <p class="fragment fade-in">The shift from counting to learning.</p>
            </section>

            <section data-transition="slide">
                <h3>Word Embeddings: The Semantic Breakthrough</h3>
                <ul>
                    <li class="fragment fade-in"><p>Words as vectors in semantic space.  </p></li>
                    <li class="fragment fade-in"><p>Each word becomes a dense vector of real numbers</p></li>
                    <li class="fragment fade-in"><p><strong>Key insight:</strong> Words with similar meanings have similar vector positions.</p></li>
                </ul>
            </section>

            <section data-transition="slide">
                <h3>What Does an N-Dimensional Vector Look Like?</h3>
                
                <div class="fragment fade-in" style="width: 100%; margin: 0 auto;">
                    <pre><code class="python"># Example: 300-dimensional word vector for "queen"
queen_vector = [
    0.2341,  -0.1829,   0.4521,  -0.7234,   0.1892,  # dims 1-5
   -0.3456,   0.6789,  -0.2134,   0.8901,  -0.4567,  # dims 6-10
    0.1234,  -0.5678,   0.9012,  -0.3456,   0.7890,  # dims 11-15
    ...                                              # ...
   -0.2109,   0.5432,  -0.8765,   0.1098,  -0.6543   # dims 296-300
]
</code></pre>
                </div>

            </section>

            <!-- Interactive Vector Visualization -->
            <section data-transition="slide">
                <h3>Visualizing the "Queen" Vector</h3> <br/><br/>
                <div style="font-size: .5em;" class="viz-container" >
                    <!-- Vector Label -->
                    <div class="vector-label">queen =</div>

                    <!-- Vector Bracket (Left) -->
                    <div class="bracket">[</div>

                    <!-- The Vector Dimensions -->
                    <div class="vector">
                        <div class="vector-dim" data-id="dim-1">0.1</div>
                        <div class="vector-dim" data-id="dim-2">0.8</div>
                        <div class="vector-dim" data-id="dim-3">0.3</div>
                        <div class="ellipsis">. . .</div>
                        <div class="vector-dim" data-id="dim-4">0.5</div>
                        <div class="vector-dim" data-id="dim-5">0.4</div>
                    </div>

                    <!-- Vector Bracket (Right) -->
                    <div class="bracket">]</div>
                    
                    <!-- Annotations (initially hidden) -->
                    <div class="annotation" id="anno-authority">
                        royalty
                    </div>
                    <div class="annotation" id="anno-power">
                        woman
                    </div>
                </div>

                <!-- These are invisible fragments that trigger the animations -->
                <div class="fragment" data-fragment-index="1" data-target-dim="dim-2" data-target-anno="anno-authority"></div>
                <div class="fragment" data-fragment-index="2" data-target-dim="dim-4" data-target-anno="anno-power"></div>
            </section>
             
             <section data-transition="slide">
                 <h3> Word2Vec Relationships</h3>    
             <img src="./word_vec_queen.webp" alt="Word2Vec Queen" style="width: 40%; height: auto;">
             <pre class="fragment fade-in"><code class="python">vector("king") - vector("man") + vector("woman") ≈ vector("queen")</code></pre>            
             </section>
            
            <section data-transition="slide">
                <h3>The Context Problem</h3>
                <p class="fragment fade-in"><strong>Word2Vec limitation:</strong> Each word has ONE fixed vector.</p>
                <ul>
                    <li class="fragment fade-in"><p>"The <span style="color: #ff6b6b; font-weight: bold;">bank</span> is next to the river."</p></li>
                    <li class="fragment fade-in"><p>"I need to go to the <span style="color: #ff6b6b; font-weight: bold;">bank</span> to deposit a check."</p></li>
                </ul>
                <p class="fragment fade-in">The word "bank" has the same vector in both sentences ! </p>
                <h4 class="fragment fade-in">The realization: Meaning depends on context.</h4>
            </section>
            
            <section data-transition="slide">
                <h3>RNNs & LSTMs: Sequential Context Modeling</h3>
                <p class="fragment fade-in">Core innovation: Process sentences word-by-word, building a "hidden state" or running summary of everything seen so far.</p>
                <pre class="fragment fade-in"><code class="js">h_t = f(word_t, h_{t-1})</code></pre>
                <p class="fragment fade-in">Breakthrough: The same word gets different representations in different contexts!</p>
                <p class="fragment fade-in">But... they have limitations.</p>
            </section>

            <section data-transition="slide">
                <h3>RNN Limitations</h3>
                <ul style="width: 100%; margin: 0 auto;">
                    <li class="fragment fade-in"><strong>Sequential bottleneck:</strong> Must process left-to-right. No parallelization.</li>
                    <li class="fragment fade-in"><strong>Information decay:</strong> Earlier context gets "forgotten" in long sequences.</li>
                    <li class="fragment fade-in"><strong>Vanishing gradients:</strong> Difficulty learning long-range dependencies.</li>
                </ul>
                <h4 class="fragment fade-in">The question: "Can we capture context without sequential processing?"</h4>
            </section>
            
            <!-- III. The Transformer Revolution: Attention Is All You Need (10 minutes) -->
            <section>
                <h2>III. The Transformer Revolution</h2>
                <h3>Attention Is All You Need</h3>
            </section>
            
            <section data-transition="slide">
                <h3>The Paradigm Shift (2017)</h3>
                <ul style="width: 100%; margin: 0 auto;">
                    <li class="fragment fade-in"><strong>Self-attention replaces recurrence.</strong> No more sequential processing.</li>
                    <li class="fragment fade-in"><strong>Parallel processing:</strong> All positions computed simultaneously.</li>
                    <li class="fragment fade-in"><strong>No fixed context window:</strong> Attention to the entire sequence.</li>
                </ul>
            </section>

            <section data-transition="slide">
                <h3>Self-Attention: The Query-Key-Value Framework</h3>
                <p class="fragment fade-in">For each word position $i$:</p>
                <ul style="width: 100%; margin: 0 auto;">
                    <li class="fragment fade-in"><strong>Query (Q):</strong> "What am I looking for?"</li>
                    <li class="fragment fade-in"><strong>Key (K):</strong> "What do I represent?"</li>
                    <li class="fragment fade-in"><strong>Value (V):</strong> "What information do I carry?"</li>
                </ul>
                <pre class="fragment fade-in"><code class="latex">Attention(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V</code></pre>
            </section>

            <section data-transition="slide">
                <h3>Why This Works</h3>
                <ul style="width: 100%; margin: 0 auto;">
                    <li class="fragment fade-in"><strong>Dynamic context:</strong> Each word's representation depends on the entire sequence.</li>
                    <li class="fragment fade-in"><strong>Relationship modeling:</strong> Direct connections between any two positions.</li>
                    <li class="fragment fade-in"><strong>Scalable learning:</strong> Attention weights are learned, not hand-crafted.</li>
                </ul>
            </section>

            <!-- IV. LLMs vs. N-grams: The Fundamental Differences (5 minutes) -->
            <section>
                <h2>IV. LLMs vs. N-grams: The Fundamental Differences</h2>
            </section>

            <section data-transition="fade">
                <h3>Side-by-Side Comparison</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Aspect</th>
                            <th>N-grams</th>
                            <th>LLMs</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr class="fragment fade-in">
                            <td>Context</td>
                            <td>Fixed window (3-5 words)</td>
                            <td>Entire sequence (thousands of tokens)</td>
                        </tr>
                        <tr class="fragment fade-in">
                            <td>Processing</td>
                            <td>Statistical counting</td>
                            <td>Neural pattern recognition</td>
                        </tr>
                        <tr class="fragment fade-in">
                            <td>Representation</td>
                            <td>Discrete word IDs</td>
                            <td>Dense vector embeddings</td>
                        </tr>
                        <tr class="fragment fade-in">
                            <td>Relationships</td>
                            <td>Adjacent words only</td>
                            <td>Any-to-any attention</td>
                        </tr>
                        <tr class="fragment fade-in">
                            <td>Learning</td>
                            <td>Frequency statistics</td>
                            <td>Gradient-based optimization</td>
                        </tr>
                        <tr class="fragment fade-in">
                            <td>Generalization</td>
                            <td>Interpolation only</td>
                            <td>Emergent reasoning</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section data-transition="slide">
                <h3>From Prediction to Generation: The Sampling Revolution</h3>
                <p class="fragment fade-in">Models don't always pick the single most likely next word—they can sample from a distribution of possible words.</p>
                <p class="fragment fade-in">This gives us <strong>controllable creativity</strong>.</p>
                <aside class="notes">
                    This is a key part for the audience to understand why LLMs feel so different and powerful.
                </aside>
            </section>

            <section data-transition="slide">
                <h3>Controlling Creativity</h3>
                <div style="display: flex; gap: 3vw; width: 100%; margin: 0 auto; flex-direction: row; align-items: flex-start;font-size: 36px;">
                    <div style="width: 50%; word-wrap: break-word; overflow-wrap: break-word;" class="fragment fade-in">
                        <h4>Temperature (τ)</h4>
                        <p>Controls randomness in the probability distribution.</p>
                        <ul>
                            <li><strong>Low (e.g., 0.1):</strong> Focused, deterministic, "safe."</li>
                            <li><strong>High (e.g., 1.2):</strong> Creative, diverse, sometimes surprising.</li>
                        </ul>
                        <pre style="margin-top: 1vh;"><code class="latex">p_i = \exp(logit_i/\tau) / \Sigma\exp(logit_j/\tau)</code></pre>
                    </div>
                    <div style="width: 50%; word-wrap: break-word; overflow-wrap: break-word;" class="fragment fade-in">
                        <h4>Top-p (Nucleus Sampling)</h4>
                        <p>Dynamic vocabulary pruning.</p>
                        <ul>
                            <li><strong>Concept:</strong> Pick from the smallest set of words whose cumulative probability ≥ p.</li>
                            <li><strong>Advantage:</strong> Adapts to context—narrow when confident, broader when uncertain.</li>
                        </ul>
                        <pre style="margin-top: 1vh;"><code class="plaintext">Example: p=0.9 → top 90% probability mass</code></pre>
                    </div>
                </div>
            </section>

            <!-- V. The Scaling Revolution & Modern Capabilities (2 minutes) -->
            <section>
                <h2>V. The Scaling Revolution</h2>
            </section>

            <section data-transition="slide">
                <h3>The Scaling Laws Discovery</h3>
                <p class="fragment fade-in">More parameters + more data + more compute = better capabilities.</p>
                <p class="fragment fade-in">This led to <strong>Emergent Abilities</strong>—capabilities that appear suddenly at scale.  </p>
                <ul class="fragment fade-in">
                    <li>In-context learning</li>
                    <li>Chain-of-thought reasoning</li>
                    <li>Code generation</li>
                </ul>
            </section>

            <!-- Closing: The Bigger Picture (2 minutes) -->
            <section>
                <h3>Final Thoughts</h3>
                <ul style="width: 80vw; margin: 0 auto; text-align: left;">
                    <li class="fragment fade-in"><strong>Attention changed everything:</strong> From sequential to parallel, local to global.</li>
                    <li class="fragment fade-in"><strong>Scale unlocks emergence:</strong> Quantitative changes lead to qualitative leaps.</li>
                    <li class="fragment fade-in"><strong>We're still early:</strong> Current LLMs are just the beginning.</li>
                </ul>
            </section>

            <section data-transition="zoom">
                <h3>The Philosophical Shift</h3>
                <p class="fragment fade-in" style="font-size: 1.5em; font-style: italic;">From "programming intelligence" to "growing intelligence."</p>
                <p class="fragment fade-in" style="font-size: 1.5em; font-style: italic;">From rule-based systems to learned representations.</p>
                <p class="fragment fade-in" style="font-size: 1.5em; font-style: italic;">"We didn't just build better autocomplete—we discovered how to make machines learn the structure of human knowledge itself, with controllable creativity."</p>
            </section>
            
            <section>
                <h2>Thank You</h2>
                <p>Questions?</p>
            </section>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.js"></script>
    <script>
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,
            transition: 'slide', // none/fade/slide/convex/concave/zoom
            transitionSpeed: 'default', // default/fast/slow
            fragments: true,
            hash: true,
            pdfSeparateFragments: false,
            // Plugins
            plugins: [ RevealHighlight ],
            // Add a progress bar to the bottom
            progress: true,
        });

        // Word2Vec Vector Visualization Handlers
        function handleFragment(event, action) {
            const fragment = event.fragment;
            const dimId = fragment.dataset.targetDim;
            const annoId = fragment.dataset.targetAnno;
            
            if (dimId && annoId) {
                const dimElement = document.querySelector(`.vector-dim[data-id="${dimId}"]`);
                const annoElement = document.getElementById(annoId);

                if(dimElement && annoElement){
                    if(action === 'add'){
                        dimElement.classList.add('highlight');
                        annoElement.classList.add('visible');
                    } else {
                        dimElement.classList.remove('highlight');
                        annoElement.classList.remove('visible');
                    }
                }
            }
        }

        Reveal.on('fragmentshown', event => {
           handleFragment(event, 'add');
        });

        Reveal.on('fragmenthidden', event => {
           handleFragment(event, 'remove');
        });
    </script>
</body>
</html>

