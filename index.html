<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>From Words to Meaning: The Evolution to LLMs</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/theme/black.css" id="theme">
    <!-- Highlight.js for code blocks -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- FIX: Added the Reveal.js highlight plugin script -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/highlight/highlight.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;700&family=Kalam:wght@400&display=swap');
        body {
            font-family: 'Inter', sans-serif;
        }

        .reveal .slides section h1,
        .reveal .slides section h2,
        .reveal .slides section h3,
        .reveal .slides section h4 {
            font-family: 'Inter', sans-serif;
            text-transform: none;
        }

        .reveal .slides section .title-large {
            font-size: 4vw;
            text-transform: uppercase;
            font-weight: 700;
            letter-spacing: 0.2vw;
            color: #4dc2f5; /* A vibrant blue accent */
            text-shadow: 0.2vw 0.2vw 0.5vw rgba(0,0,0,0.5);
            line-height: 1.1;
            margin: 0.2em 0;
        }

        .reveal .slides section p {
            line-height: 1.4;
        }
        
        .reveal .slides section .hook-text {
            color: #f5f5f5;
            font-size: 1.5em;
            line-height: 1.2;
            font-weight: 300;
            text-align: center;
        }

        .reveal pre {
            border: 0.1vw solid #4dc2f5;
            box-shadow: 0 0 1vw rgba(77, 194, 245, 0.5);
            padding: 1.5vw;
            border-radius: 0.8vw;
        }
        
        .reveal code {
            font-size: 0.9em;
            background-color: #1a1a1a;
            border-radius: 0.5vw;
        }

        .reveal .progress span {
            background: #4dc2f5;
        }

        .reveal .controls button {
            color: #4dc2f5;
        }

        .comparison-table {
            width: 100%;
            margin: 20px 0;
            border-collapse: collapse;
            font-size: 0.72em;
        }

        .comparison-table th, .comparison-table td {
            border: 0.1vw solid #444;
            padding: 0.6vw 1vw;
            text-align: left;
            white-space: nowrap;
            vertical-align: top;
        }

        .comparison-table th {
            background-color: #222;
            color: #4dc2f5;
            font-weight: 700;
        }

        .comparison-table td {
            background-color: #1a1a1a;
        }
        
        .code-title {
            text-align: left;
            font-size: 0.8em;
            color: #aaa;
            margin-bottom: 5px;
        }

        /* Word2Vec Vector Visualization Styles */
        .viz-container {
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin-top: 3vh;
            scale: 1.2;
        }

        .vector-label {
            font-family: 'Kalam', cursive;
            font-size: 2em;
        }

        .vector {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: 'Kalam', cursive;
            font-size: 1.8em;
        }

        .vector-dim {
            padding: 5px 15px;
            margin: 2px 0;
            border: 3px solid transparent;
            border-radius: 50%;
            transition: all 0.3s ease-in-out;
        }
        
        .vector-dim.highlight {
            border-color: #EF4444;
        }

        .ellipsis {
            letter-spacing: 5px;
            padding: 10px 0;
        }

        .bracket {
            font-size: 8em;
            font-weight: 100;
            color: #9CA3AF;
        }

        .annotation {
            position: absolute;
            font-family: 'Kalam', cursive;
            font-size: 1.5em;
            opacity: 0;
            visibility: hidden;
            transition: all 0.4s ease-in-out;
            color: #9CA3AF;
        }

        .annotation.visible {
            opacity: 1;
            visibility: visible;
        }
        
        #anno-authority {
            top: 65px;
            left: 65%;
        }

        #anno-power {
            bottom: 15%;
            left: 65%;
        }
        
        .annotation svg {
            position: absolute;
            top: 50%;
            right: 100%;
            width: 100px;
            height: 50px;
            transform: translateY(-50%);
            overflow: visible;
        }

        #anno-power svg {
            width: 150px;
        }

        .annotation path {
            stroke: #9CA3AF;
            stroke-width: 2;
            fill: none;
            stroke-dasharray: 200;
            stroke-dashoffset: 200;
            transition: stroke-dashoffset 0.8s ease-in-out;
        }

        .annotation.visible path {
            stroke-dashoffset: 0;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Opening Hook (2 minutes) -->
            <section data-transition="zoom">
                <!-- Brain image in top right corner
                <div style="position: absolute; top: 2vh; right: 2vw; z-index: 10;">
                    <img src="./llm_brain_title.jpg" alt="LLM Brain Visualization" style="width: 15vw; height: auto; border-radius: 1vw; box-shadow: 0 0 1.5vw rgba(77, 194, 245, 0.3); border: 0.1vw solid #4dc2f5; opacity: 0.8;">
                </div> -->
                
                <h2 class="title-large">The Road  &nbsp;</h2>
                <h2 class="title-large">to LLMs</h2>
                <h3>From a Single Neuron to a Global Brain</h3>
                <p class="fragment fade-in-then-out">Singapore Technology Week 2025</p>
                
                <aside class="notes">
                    Hello everyone, I'm thrilled to be here. Today, we're going to explore one of the most incredible transformations in recent memory - the evolution from simple neurons to complex language models.
                </aside>
            </section>
            <section data-transition="slide">
                <p class="hook-text">About Me</p>
                <div style="display: flex; align-items: center; justify-content: center; gap: 4vw;">
                    <div style="flex: 1; text-align: center;">
                        <img src="./SachinProfile.jpg" alt="Sachin Chopra" style="width: 12vw; height: 12vw; border-radius: 50%; object-fit: cover; border: 0.3vw solid #4dc2f5; box-shadow: 0 0 2vw rgba(77, 194, 245, 0.3);">
                    </div>
                    <div style="flex: 2;">
                        <ul style="text-align: left;">
                            <li>Sachin Chopra</li>
                            <li>As a Fullstack Engineer, I build complex systems.</li>
                            <li>To me, LLMs are the very fascinating systems, and I'm excited to share their origin story.</li>
                        </ul>
                    </div>
                </div>
            </section>
            <section data-transition="slide">
                <p class="hook-text">About the Talk</p>
                I'll be talking about the evolution of language models <br/><br/>
                <ul style="text-align: left;">
                    <li><strong style="color: #ff6b6b;">The Age of Counting:</strong> We'll start with the simple but powerful idea of statistical prediction (N-grams).</li><br/>
                    <li><strong style="color: #ff6b6b;">The Age of Meaning:</strong> We'll witness the breakthrough of giving words meaning and memory (Embeddings & RNNs).</li><br/>
                </ul>
            </section>

            <section data-transition="slide">
                <ul style="text-align: left;">
                    <li><strong style="color: #ff6b6b;">The Age of Attention:</strong> We'll explore the paradigm shift that allowed models to see the whole picture at once (Transformers).</li><br/>
                    <li><strong style="color: #ff6b6b;">The Age of Scale:</strong> Finally, we'll see how pushing these ideas to their limits created the LLMs we know today.</li>
                </ul>
            </section>

            <section data-transition="slide">
                <div style="width: 100%; margin: 0 auto;">
                    <pre><code class="plaintext">"I flew from London to Singapore last week. 
I'm really enjoying my stay in ______."
</code></pre>
                </div>
            </section>            
            <section data-transition="slide">
                <h2 class="hook-text">How did we go from a simple autocomplete...</h2>
                <img src="./bad_autocomplete.jpg" alt="Bad autocomplete" style="width: 70%; height: auto;">
            </section>
            
            <section data-transition="slide">
                <h2 class="hook-text">...to machines that seem to understand context, meaning, and even reasoning?</h2>
                <img src="./gpt_completion.png" alt="ChatGPT Completion" style="width: 100%; height: auto;">
            </section>

            <!-- I. The Foundation Era: Statistical NLP (5 minutes) -->
            <section>
                <h2>I. The Foundation Era: Statistical NLP</h2>
                <p class="fragment fade-in">1950s-2000s</p>
            </section>

            <section data-transition="slide">
                <h3>N-gram Models: The Counting Approach</h3>
                <p >Core principle based on frequency counting:</p>
                <pre><code class="latex">P(word | previous n-1 words)</code></pre>
                <pre class="fragment fade-in"><code class="python"># Simple N-gram counting
data = ["the cat sat on the mat", "the dog sat on the rug"]
# Probabilities: {('the', 'cat'): 1, ('cat', 'sat'): 1, ...}</code></pre>
            </section>

            <section data-transition="slide">
                <h3>N-gram Strengths & Fatal Flaws</h3>
                <ul style="width: 100%; margin: 0 auto;">
                    <li class="fragment fade-in"><strong style="color: #ff6b6b;">Strengths:</strong> Simple, interpretable, computationally efficient.</li>
                    <li class="fragment fade-in"><strong style="color: #ff6b6b;">Limitations:</strong> Statistical patterns, no semantic understanding.</li>
                </ul>
            </section>

            <section data-transition="slide">
                <h3>The 'Singapore' Problem</h3>
                <div style="width: 100%; margin: 0 auto;">
                    <pre><code class="plaintext">"I flew from London to Singapore last week. 
I'm really enjoying my stay in ______."

-> Paris
-> London
-> New York</code></pre>
                </div>
            </section>

            
            
            <section data-transition="slide transition-hook">
                <p class="hook-text">What if words could have meaning, not just frequency?</p>
            </section>
            
            <section>
                <h2>II. The Neural Revolution: Learning Representations</h2>
                <p class="fragment fade-in">The shift from counting to learning.</p>
            </section>

            <section data-transition="slide">
                <h3>Word Embeddings: The Semantic Breakthrough</h3>
                <ul>
                    <li class="fragment fade-in"><p>Words as vectors in semantic space.  </p></li>
                    <li class="fragment fade-in"><p>Each word becomes a dense vector of real numbers</p></li>
                    <li class="fragment fade-in"><p><strong>Key insight:</strong> Words with similar meanings have similar vector positions.</p></li>
                </ul>
            </section>

            <section data-transition="slide">
                <h3>What Does an N-Dimensional Vector Look Like?</h3>
                
                <div class="fragment fade-in" style="width: 100%; margin: 0 auto;">
                    <pre><code class="python"># Example: 300-dimensional word vector for "queen"
queen_vector = [
    0.2341,  -0.1829,   0.4521,  -0.7234,   0.1892,  # dims 1-5
   -0.3456,   0.6789,  -0.2134,   0.8901,  -0.4567,  # dims 6-10
    0.1234,  -0.5678,   0.9012,  -0.3456,   0.7890,  # dims 11-15
    ...                                              # ...
   -0.2109,   0.5432,  -0.8765,   0.1098,  -0.6543   # dims 296-300
]
</code></pre>
                </div>

            </section>

            <!-- Interactive Vector Visualization -->
            <section data-transition="slide">
                <h3>Visualizing the "Queen" Vector</h3> <br/><br/>
                <div style="font-size: .5em;" class="viz-container" >
                    <!-- Vector Label -->
                    <div class="vector-label">queen =</div>

                    <!-- Vector Bracket (Left) -->
                    <div class="bracket">[</div>

                    <!-- The Vector Dimensions -->
                    <div class="vector">
                        <div class="vector-dim" data-id="dim-1">0.1</div>
                        <div class="vector-dim" data-id="dim-2">0.8</div>
                        <div class="vector-dim" data-id="dim-3">0.3</div>
                        <div class="ellipsis">. . .</div>
                        <div class="vector-dim" data-id="dim-4">0.5</div>
                        <div class="vector-dim" data-id="dim-5">0.4</div>
                    </div>

                    <!-- Vector Bracket (Right) -->
                    <div class="bracket">]</div>
                    
                    <!-- Annotations (initially hidden) -->
                    <div class="annotation" id="anno-authority">
                        royalty
                    </div>
                    <div class="annotation" id="anno-power">
                        woman
                    </div>
                </div>

                <!-- These are invisible fragments that trigger the animations -->
                <div class="fragment" data-fragment-index="1" data-target-dim="dim-2" data-target-anno="anno-authority"></div>
                <div class="fragment" data-fragment-index="2" data-target-dim="dim-4" data-target-anno="anno-power"></div>
            </section>
             
             <section data-transition="slide">
                 <h3> Words had Semantic Relationships</h3>    
             <img src="./word_vec_queen.webp" alt="Word2Vec Queen" style="width: 40%; height: auto;">
             <pre class="fragment fade-in"><code class="python">vector("king") - vector("man") + vector("woman") ‚âà vector("queen")</code></pre>            
             </section>
            
            <section data-transition="slide">
                <h3>The Context Problem</h3>
                <p class="fragment fade-in"><strong>Word2Vec limitation:</strong> Each word has ONE fixed vector.</p>
                <ul>
                    <li class="fragment fade-in"><p>"The <span style="color: #ff6b6b; font-weight: bold;">bank</span> is next to the river."</p></li>
                    <li class="fragment fade-in"><p>"I need to go to the <span style="color: #ff6b6b; font-weight: bold;">bank</span> to deposit a check."</p></li>
                </ul>
                <p class="fragment fade-in">The word "bank" has the same vector in both sentences ! </p>
                <h4 class="fragment fade-in">The realization: Meaning depends on context.</h4>
            </section>

            <!-- <section>
            <pre><code class="plaintext" data-trim data-noescape>
                "I flew from London to Singapore last week. 
                I'm really enjoying my stay in ______."
                </code></pre>
                <hr>

                <div class="fragment fade-in" data-fragment-index="1">
                    <p><strong>The Breakthrough üí°:</strong> It learns words exist in semantic "neighborhoods".</p>
                    <pre><code class="python" data-trim data-noescape>
                        - "Singapore" vector: [0.2, -0.1, 0.8, 0.3, ...]
                        - Similar words: ["Malaysia": 0.89, "Thailand": 0.85, "Asia": 0.82]
                        - But still predicts: ‚ùå "Malaysia" or "Asia"
                        
                        The Problem: No context awareness - "Singapore" is just a static vector
                    </code></pre>
                </div>
                
                <div class="fragment fade-in" data-fragment-index="2">
                    <p style="margin-top: 25px;"><strong>The Flaw ‚ùå:</strong> It has no memory of the sentence's sequence.</p>
                    <p>The vector for "Singapore" is <strong>static</strong>. The model has no special reason to pick the word that appeared 5 words ago.
                    </p>
                </div>

                <div class="fragment fade-in" data-fragment-index="3">
                    <blockquote style="width: 80%; margin: 20px auto; text-align: left;">
                       <strong>The Core Problem:</strong> One word, one vector. It can't tell the difference between a "river bank" and a "money bank."
                    </blockquote>
                </div>
            </section>              -->

            <section data-transition="slide transition-hook">
                <h3 class="hook-text"> Each word has fixed meaning regardless of context. <br/> How can we understand context for each word?</h3>
            </section>
            <section data-transition="slide">
                <h3>RNNs Sequential Context Modeling</h3>
                <p class="fragment fade-in">Core innovation: Process sentences word-by-word, building a "hidden state" or running summary of everything seen so far.</p>
                <p class="fragment fade-in">Breakthrough: The same word gets different representations in different contexts!</p>
            </section>            
            <section data-transition="slide">
                <div style="display: flex; flex-direction: row; align-items: center; justify-content: center; gap: 2%;">
                <div style="width: 45%; margin: 0 auto;">
                    <pre><code class="python">
h‚ÇÅ = RNN(x‚ÇÅ, h‚ÇÄ)    ‚Üí ‚ÄúI‚Äù  
h‚ÇÇ = RNN(x‚ÇÇ, h‚ÇÅ)    ‚Üí ‚Äúflew‚Äù  
h‚ÇÉ = RNN(x‚ÇÉ, h‚ÇÇ)    ‚Üí ‚Äúfrom‚Äù  
...
h‚ÇÅ‚ÇÖ = RNN(x‚ÇÅ‚ÇÖ, h‚ÇÅ‚ÇÑ) ‚Üí ‚Äúhere‚Äù                
                    </code></pre>
                </div>
                <div>
                <p style="text-align: left;">At each time step t, the RNN receives:
                    <ul>
                        <li>The current word vector x‚Çú</li>
                        <li>The hidden state from the previous step <code>h‚Çú‚Çã‚ÇÅ</code></li>
                    </ul><br/>
                </p>
                <p style="text-align: left;">
                    <ul class="fragment fade-in">And it outputs:
                        <li>A new <span style="color: #ff6b6b;">hidden state <code>h‚Çú</code></span> that captures all context up to and including this word.</li>
                    </ul>
                </p>
                </div>
            </div>
            </section>

            <section data-transition="slide">
                <pre><code class="python">
Word:       I ‚Üí flew ‚Üí from ‚Üí London ‚Üí ... ‚Üí here
Embedding:  v1   v2     v3     v4     ...    v15
Hidden:     h1   h2     h3     h4     ...    h15

                </code></pre>
            </section>
            <section data-transition="slide">
                <h3>Controlling Creativity</h3>
                <p class="fragment fade-in">Models don't always pick the single most likely next word‚Äîthey can sample from a distribution of possible words.</p>
            </section>
            <section data-transition="slide">
            <p>
                With context captured - the model starts understanding context a <i>'little'</i>
                <pre class="fragment fade-in"><code class="python">"I flew from London to Singapore last week. 
I'm really enjoying my stay in ______."</code></pre>                
            <ul class="fragment fade-in">Likely top predictions:
                <li>‚úÖ ‚ÄúSingapore‚Äù ‚Äî based on prior sentence.</li>
                <li>‚ùå ‚ÄúLondon‚Äù ‚Äî possible if model misunderstood context.</li>
                <li>‚ùå ‚Äúhotel‚Äù ‚Äî grammatically correct, but less likely.</li>                
            </ul>
            </p>
            </section>

            <section data-transition="slide">
                <h3>RNN Limitations</h3>
                <ul style="width: 100%; margin: 0 auto;">
                    <li ><strong class="fragment highlight-red">Sequential bottleneck:</strong> Must process left-to-right. No parallelization.</li>
                    <li ><strong class="fragment highlight-red">Information decay:</strong> Earlier context gets "forgotten" in long sequences.</li>
                </ul>
            </section>

            <section data-transition="slide transition-hook"
                <p class="fragment fade-in hook-text">Can we capture long distance context efficiently without sequential processing?</p>
            </section>
            
            <!-- III. The Transformer Revolution: Attention Is All You Need (10 minutes) -->
             <section data-transition="slide">
                "Instead of relying only on the hidden state, let's allow the model to 'look back' and pay direct <strong style="color: #ff6b6b;">'attention'</strong> to the most relevant input words at each step of the output."
            </section>
            <section data-transition="slide">
                <h3>The Paradigm Shift (2017)</h3>
                <img src="./attention.png" alt="Attention" style="width: 70%; height: auto;">
            </section>
            <section data-transition="slide">
                <h2>III. The Transformer Revolution</h2>
                <h3><strong style="color: #ff6b6b;">(Self)Attention</strong> Is All You Need</h3>
                
            </section>
            <section data-transition="slide">
                <h3>Self-Attention</h3>
                <ul style="width: 100%; margin: 0 auto;">For each word position i, compute:
                    <li class="fragment fade-in"><strong>Query (Q):</strong> "What am I looking for?"</li>
                    <li class="fragment fade-in"><strong>Key (K):</strong> "What do I represent?"</li>
                    <li class="fragment fade-in"><strong>Value (V):</strong> "What information do I carry?"</li>
                </ul>
            </section>            
            
            <section data-transition="slide">
                <ul style="width: 100%; margin: 0 auto;">
                    <strong>Self-attention enables.</strong>
                    <li class="fragment fade-in"><strong>Parallel processing:</strong> All positions computed simultaneously.</li>
                    <li class="fragment fade-in"><strong>No fixed context window:</strong> Attention to the entire sequence.</li>
                </ul>
            </section>

            <section data-transition="slide">
                <p><strong>If transformers process in parallel, how do they know the order of words?</strong></p><br/>
                <p>Add special "position vectors" to each word</p>
<pre><code class="python">
| Token  | Word vector | Positional vector| Combined Vector|
| ------ | ----------- | ---------------- | -------------- |
| "I"    | v_I         | p_1              | v_I + p_1      |
| "flew" | v_flew      | p_2              | v_flew + p_2   |
| "from" | v_from      | p_3              | v_from + p_3   |

</code></pre>
            </section>

            <section data-transition="slide">
                <h3>Transformer Analysis: Context Understood!</h3>
                
                <pre><code class="plaintext" data-trim data-noescape>
                "I flew from London to Singapore last week. 
                I'm really enjoying my stay in ______."
                </code></pre>
                </section>
                <section data-transition="slide">
                <p class="fragment fade-in">word "stay" pays <strong> strong attention</strong> to Singapore:</p>
                
                <div class="r-stack">
                    <pre class="fragment fade-in current-only" data-fragment-index="2"><code class="yaml" data-trim data-noescape>
                    Self-Attention Heatmap for "stay":
                      "I":         0.05   # Subject (low relevance)
                      "flew":      0.15   # Travel context
                      "from":      0.10   # Direction indicator
                      "London":    0.05   # Origin (not current location)
                      "to":        0.20   # Direction to destination
                      "Singapore": 0.85   # üéØ The Destination = Current Location!
                      "last":      0.10   # Time context
                      "week":      0.08   # Time context
                    </code></pre>
                </div>

                <h4 class="fragment fade-in" style="margin-top: 30px;">Result: <span style="color: #22c55e;">‚úÖ "Singapore" (with high confidence!)</span></h4>
            </section>

            <section data-transition="slide">
                <h3>The Attention Era</h3>
                <p class="fragment fade-in">‚úÖ Direct, dynamic connections between *any* words, processed in parallel.</p>
                <p class="fragment fade-in">‚úÖ We can now model true long-range dependencies and contextual meaning.</p>
            </section>

 
            <section data-transition="slide">
                <h3>Key Question</h3>
                <p style="font-size: 1.2em; font-weight: bold;">
                  Can we simplify the Transformer architecture to generate text efficiently by focusing only on the generation task?
                </p>
              </section>
            <section data-transition="slide">
                <h3>Transformer Architecture Challenge for Text Generation</h3>
                <ul>
                  <li class="fragment fade-in"><strong>Original Transformer (2017):</strong> Built for translation</li>
                  <li class="fragment fade-in"><strong>Encoder:</strong> Understands input sentence</li>
                  <li class="fragment fade-in"><strong>Decoder:</strong> Generates output sentence</li><br/>

                  <p class="fragment fade-in">What if we just used the decoder part and made it <strong>really, really good?</strong></p>
                </ul>
              </section>
              <section data-transition="slide">
                <h3>GPT‚Äôs Decoder-Only Approach</h3>
                <p><strong>The Insight:</strong> For text generation, decoder-only is enough!</p>
                <ul>
                  <li class="fragment fade-in"><strong>Simpler architecture</strong> = easier to scale</li>
                  <li class="fragment fade-in"><strong>All compute focused</strong> on generation task</li>
                  <li class="fragment fade-in"><strong>Masked self-attention</strong> prevents "cheating" (seeing future words)</li>
                </ul>
              </section>              

            <!-- V. The Scaling Revolution & Modern Capabilities (2 minutes) -->
            <section>
                <h2>V. The Scaling Revolution</h2>
            </section>

            <section data-transition="slide">
                <h3>The Scaling Laws Discovery</h3>
                <p style="text-align: left;" class="fragment fade-in">More parameters + more data + more compute = better capabilities.</p>
                <p style="text-align: left;" class="fragment fade-in">This led to <strong>Emergent Abilities</strong>‚Äîcapabilities that appear suddenly at scale.
                <ul style="text-align: left;" class="fragment fade-in">
                    <li>In-context learning</li>
                    <li>Chain-of-thought reasoning</li>
                    <li>Code generation</li>
                </ul>
            </p>
            </section>
            <section data-transition="slide">
                <h3>Model Scale Over Time</h3>
                <table style="width:100%; font-size: 90%; border-collapse: collapse;">
                  <thead>
                    <tr>
                      <th style="text-align: left; padding: 10px;">Model</th>
                      <th style="text-align: left; padding: 10px;">Parameters</th>
                      <th style="text-align: left; padding: 10px;">Year</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td style="padding: 10px;"><strong>GPT-1</strong></td>
                      <td style="padding: 10px; color: #4dc2f5;">117 million</td>
                      <td style="padding: 10px;">2018</td>
                    </tr>
                    <tr>
                      <td style="padding: 10px;"><strong>GPT-2</strong></td>
                      <td style="padding: 10px;">1.5 billion</td>
                      <td style="padding: 10px;">2019</td>
                    </tr>
                    <tr>
                      <td style="padding: 10px;"><strong>GPT-3</strong></td>
                      <td style="padding: 10px;" class="fragment highlight-red">175 billion</td>
                      <td style="padding: 10px;">2020</td>
                    </tr>
                  </tbody>
                </table>
              </section>            

            <!-- Closing: The Bigger Picture (2 minutes) -->
            <section data-transition="slide">
                <h3>Final Takeaways</h3>
                <ul>
                    <li><strong class="fragment highlight-red">Vector Embeddings:</strong> Capture semantic meaning of words and concepts.<br/></li><br/>
                    <li><strong class="fragment highlight-red">Attention Mechanisms:</strong> Enable models to focus on relevant parts of input for building contextual understanding.<br/></li><br/>
                    <li><strong class="fragment highlight-red">Scale:</strong> With increased data and parameters, models started to generalize the structure of human knowledge - to become a <strong> "global brain" </strong> that can reason, code, and create.</strong></p>
                </ul>
            </section>            
            <section>
                <h2>Thank You</h2>
                <!-- QR Code Placeholder -->
                <div style="margin: 2em auto; text-align: center;">
                    <div;">
                        <img src="./qrcode_sach2211.vercel.app.png" style="width: 30vh; min-width: 300px; max-width: 400px; height: auto;"/>
                        <br/>
                    </div>
                    <p style="font-size: 0.8em; margin-top: 1em; color: #aaa;">
                        If you want to reach out or get a link to the slides, can scan this QR code
                    </p>
                </div>
            </section>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.js"></script>
    <script>
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,
            transition: 'slide', // none/fade/slide/convex/concave/zoom
            transitionSpeed: 'default', // default/fast/slow
            fragments: true,
            hash: true,
            pdfSeparateFragments: false,
            // Plugins
            plugins: [ RevealHighlight ],
            // Add a progress bar to the bottom
            progress: true,
        });

        // Word2Vec Vector Visualization Handlers
        function handleFragment(event, action) {
            const fragment = event.fragment;
            const dimId = fragment.dataset.targetDim;
            const annoId = fragment.dataset.targetAnno;
            
            if (dimId && annoId) {
                const dimElement = document.querySelector(`.vector-dim[data-id="${dimId}"]`);
                const annoElement = document.getElementById(annoId);

                if(dimElement && annoElement){
                    if(action === 'add'){
                        dimElement.classList.add('highlight');
                        annoElement.classList.add('visible');
                    } else {
                        dimElement.classList.remove('highlight');
                        annoElement.classList.remove('visible');
                    }
                }
            }
        }

        Reveal.on('fragmentshown', event => {
           handleFragment(event, 'add');
        });

        Reveal.on('fragmenthidden', event => {
           handleFragment(event, 'remove');
        });
    </script>
</body>
</html>

